cd ../
CUDA_VISIBLE_DEVICES=0 HYDRA_CONFIG=./configs/polygraph_eval_sciq.yaml python run_polygraph.py ignore_exceptions=False use_density_based_ue=True batch_size=1 subsample_train_dataset=100 subsample_background_train_dataset=1000 subsample_eval_dataset=2000 model.path=meta-llama/Meta-Llama-3.1-8B +model.attn_implementation=eager cache_path=./workdir/gen_output_loo +generation_params.samples_n=5 +metric_thrs="[0.3]" +layers="[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,-1]" +max_new_tokens_1=128 +train_dataset_1="samsum" +train_text_column_1=dialogue +train_label_column_1=summary +train_prompt_1\="Here is the dialogue and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_1=train +max_new_tokens_2=56 +train_dataset_2="xsum" +train_text_column_2=document +train_label_column_2=summary +train_prompt_2\="Here is the text and it is short one-sentence summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_2=train +max_new_tokens_3=128 +train_dataset_3="['cnn_dailymail', '3.0.0']" +train_text_column_3=article +train_label_column_3=highlights +train_prompt_3\="Here is the text and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_3=train +max_new_tokens_4=20 +train_dataset_4="coqa" +train_text_column_4=questions +train_label_column_4=answers +train_prompt_4\="\n\nQuestion: \{question\}\nAnswer:\{answer\}" +train_split_4=train +train_description_4\="The following are stories and questions about them. Each story is followed by a question and answer to a given question.\n\nStory: \{story\}" +max_new_tokens_5=20 +train_dataset_5="['trivia_qa', 'rc.nocontext']" +train_text_column_5=question +train_label_column_5=answer +train_prompt_5\="Question: \{question\}\nAnswer:\{answer\}" +train_split_5=train +few_shot_split_5=train +train_n_shot_5=5 +max_new_tokens_6=128 +train_dataset_6="['truthful_qa', 'generation']" +train_text_column_6=question +train_label_column_6=correct_answers +train_prompt_6\="Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona\, Spain.\n\nQ:\{question\}\nA:" +train_split_6=validation +few_shot_split_6=validation +train_n_shot_6=0 +max_new_tokens_7=128 +train_dataset_7="bigbio/pubmed_qa" +train_text_column_7=QUESTION +train_label_column_7=LONG_ANSWER +train_prompt_7\="The following are abstract and question about them. Each abstract is followed by a question and answer to a given question. Abstract: \{context\}\nQuestion: \{question\}\nAnswer:" +train_split_7=train +train_n_shot_7=0 +max_new_tokens_8=128 +train_dataset_8="keivalya/MedQuad-MedicalQnADataset" +train_text_column_8=Question +train_label_column_8=Answer +train_prompt_8\="Question: \{question\}\nAnswer: \{answer\}" +train_split_8=train +few_shot_split_8=train +train_n_shot_8=5 +max_new_tokens_9=107 +train_dataset_9="['wmt14', 'fr-en']" +train_text_column_9=fr +train_label_column_9=en +train_prompt_9\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_9=train +max_new_tokens_10=107 +train_dataset_10="['wmt19', 'de-en']" +train_text_column_10=de +train_label_column_10=en +train_prompt_10\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_10=train +run_proposed_methods=True +clean_md_device=cuda
wait
CUDA_VISIBLE_DEVICES=0 HYDRA_CONFIG=./configs/polygraph_eval_sciq.yaml python run_polygraph.py ignore_exceptions=False use_density_based_ue=True batch_size=1 subsample_train_dataset=300 subsample_background_train_dataset=1000 subsample_eval_dataset=2000 model.path=meta-llama/Meta-Llama-3.1-8B +model.attn_implementation=eager cache_path=./workdir/gen_output_loo +generation_params.samples_n=5 +metric_thrs="[0.3]" +layers="[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,-1]" +max_new_tokens_1=128 +train_dataset_1="samsum" +train_text_column_1=dialogue +train_label_column_1=summary +train_prompt_1\="Here is the dialogue and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_1=train +max_new_tokens_2=56 +train_dataset_2="xsum" +train_text_column_2=document +train_label_column_2=summary +train_prompt_2\="Here is the text and it is short one-sentence summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_2=train +max_new_tokens_3=128 +train_dataset_3="['cnn_dailymail', '3.0.0']" +train_text_column_3=article +train_label_column_3=highlights +train_prompt_3\="Here is the text and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_3=train +max_new_tokens_4=20 +train_dataset_4="coqa" +train_text_column_4=questions +train_label_column_4=answers +train_prompt_4\="\n\nQuestion: \{question\}\nAnswer:\{answer\}" +train_split_4=train +train_description_4\="The following are stories and questions about them. Each story is followed by a question and answer to a given question.\n\nStory: \{story\}" +max_new_tokens_5=20 +train_dataset_5="['trivia_qa', 'rc.nocontext']" +train_text_column_5=question +train_label_column_5=answer +train_prompt_5\="Question: \{question\}\nAnswer:\{answer\}" +train_split_5=train +few_shot_split_5=train +train_n_shot_5=5 +max_new_tokens_6=128 +train_dataset_6="['truthful_qa', 'generation']" +train_text_column_6=question +train_label_column_6=correct_answers +train_prompt_6\="Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona\, Spain.\n\nQ:\{question\}\nA:" +train_split_6=validation +few_shot_split_6=validation +train_n_shot_6=0 +max_new_tokens_7=128 +train_dataset_7="bigbio/pubmed_qa" +train_text_column_7=QUESTION +train_label_column_7=LONG_ANSWER +train_prompt_7\="The following are abstract and question about them. Each abstract is followed by a question and answer to a given question. Abstract: \{context\}\nQuestion: \{question\}\nAnswer:" +train_split_7=train +train_n_shot_7=0 +max_new_tokens_8=128 +train_dataset_8="keivalya/MedQuad-MedicalQnADataset" +train_text_column_8=Question +train_label_column_8=Answer +train_prompt_8\="Question: \{question\}\nAnswer: \{answer\}" +train_split_8=train +few_shot_split_8=train +train_n_shot_8=5 +max_new_tokens_9=107 +train_dataset_9="['wmt14', 'fr-en']" +train_text_column_9=fr +train_label_column_9=en +train_prompt_9\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_9=train +max_new_tokens_10=107 +train_dataset_10="['wmt19', 'de-en']" +train_text_column_10=de +train_label_column_10=en +train_prompt_10\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_10=train +run_proposed_methods=True +clean_md_device=cuda
wait
CUDA_VISIBLE_DEVICES=0 HYDRA_CONFIG=./configs/polygraph_eval_sciq.yaml python run_polygraph.py ignore_exceptions=False use_density_based_ue=True batch_size=1 subsample_train_dataset=100 subsample_background_train_dataset=1000 subsample_eval_dataset=2000 model.path=meta-llama/Meta-Llama-3.1-8B +model.attn_implementation=eager cache_path=./workdir/gen_output_loo +generation_params.samples_n=5 +metric_thrs="[0.3]" +layers="[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,-1]" +max_new_tokens_1=128 +train_dataset_1="samsum" +train_text_column_1=dialogue +train_label_column_1=summary +train_prompt_1\="Here is the dialogue and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_1=train +max_new_tokens_2=56 +train_dataset_2="xsum" +train_text_column_2=document +train_label_column_2=summary +train_prompt_2\="Here is the text and it is short one-sentence summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_2=train +max_new_tokens_3=128 +train_dataset_3="['cnn_dailymail', '3.0.0']" +train_text_column_3=article +train_label_column_3=highlights +train_prompt_3\="Here is the text and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_3=train +max_new_tokens_4=20 +train_dataset_4="coqa" +train_text_column_4=questions +train_label_column_4=answers +train_prompt_4\="\n\nQuestion: \{question\}\nAnswer:\{answer\}" +train_split_4=train +train_description_4\="The following are stories and questions about them. Each story is followed by a question and answer to a given question.\n\nStory: \{story\}" +max_new_tokens_5=20 +train_dataset_5="['trivia_qa', 'rc.nocontext']" +train_text_column_5=question +train_label_column_5=answer +train_prompt_5\="Question: \{question\}\nAnswer:\{answer\}" +train_split_5=train +few_shot_split_5=train +train_n_shot_5=5 +max_new_tokens_6=128 +train_dataset_6="['truthful_qa', 'generation']" +train_text_column_6=question +train_label_column_6=correct_answers +train_prompt_6\="Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona\, Spain.\n\nQ:\{question\}\nA:" +train_split_6=validation +few_shot_split_6=validation +train_n_shot_6=0 +max_new_tokens_7=128 +train_dataset_7="bigbio/pubmed_qa" +train_text_column_7=QUESTION +train_label_column_7=LONG_ANSWER +train_prompt_7\="The following are abstract and question about them. Each abstract is followed by a question and answer to a given question. Abstract: \{context\}\nQuestion: \{question\}\nAnswer:" +train_split_7=train +train_n_shot_7=0 +max_new_tokens_8=128 +train_dataset_8="keivalya/MedQuad-MedicalQnADataset" +train_text_column_8=Question +train_label_column_8=Answer +train_prompt_8\="Question: \{question\}\nAnswer: \{answer\}" +train_split_8=train +few_shot_split_8=train +train_n_shot_8=5 +max_new_tokens_9=107 +train_dataset_9="['wmt14', 'fr-en']" +train_text_column_9=fr +train_label_column_9=en +train_prompt_9\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_9=train +max_new_tokens_10=107 +train_dataset_10="['wmt19', 'de-en']" +train_text_column_10=de +train_label_column_10=en +train_prompt_10\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_10=train +run_baselines=False +run_eigenscore=False +run_layerwise_methods=True +md_device=cuda
wait
CUDA_VISIBLE_DEVICES=0 HYDRA_CONFIG=./configs/polygraph_eval_sciq.yaml python run_polygraph.py ignore_exceptions=False use_density_based_ue=True batch_size=1 subsample_train_dataset=300 subsample_background_train_dataset=1000 subsample_eval_dataset=2000 model.path=meta-llama/Meta-Llama-3.1-8B +model.attn_implementation=eager cache_path=./workdir/gen_output_loo +generation_params.samples_n=5 +metric_thrs="[0.3]" +layers="[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,-1]" +max_new_tokens_1=128 +train_dataset_1="samsum" +train_text_column_1=dialogue +train_label_column_1=summary +train_prompt_1\="Here is the dialogue and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_1=train +max_new_tokens_2=56 +train_dataset_2="xsum" +train_text_column_2=document +train_label_column_2=summary +train_prompt_2\="Here is the text and it is short one-sentence summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_2=train +max_new_tokens_3=128 +train_dataset_3="['cnn_dailymail', '3.0.0']" +train_text_column_3=article +train_label_column_3=highlights +train_prompt_3\="Here is the text and it is short summary.\n\nText:\n\{text\}\n\nSummary \(one sentence\):\n" +train_split_3=train +max_new_tokens_4=20 +train_dataset_4="coqa" +train_text_column_4=questions +train_label_column_4=answers +train_prompt_4\="\n\nQuestion: \{question\}\nAnswer:\{answer\}" +train_split_4=train +train_description_4\="The following are stories and questions about them. Each story is followed by a question and answer to a given question.\n\nStory: \{story\}" +max_new_tokens_5=20 +train_dataset_5="['trivia_qa', 'rc.nocontext']" +train_text_column_5=question +train_label_column_5=answer +train_prompt_5\="Question: \{question\}\nAnswer:\{answer\}" +train_split_5=train +few_shot_split_5=train +train_n_shot_5=5 +max_new_tokens_6=128 +train_dataset_6="['truthful_qa', 'generation']" +train_text_column_6=question +train_label_column_6=correct_answers +train_prompt_6\="Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona\, Spain.\n\nQ:\{question\}\nA:" +train_split_6=validation +few_shot_split_6=validation +train_n_shot_6=0 +max_new_tokens_7=128 +train_dataset_7="bigbio/pubmed_qa" +train_text_column_7=QUESTION +train_label_column_7=LONG_ANSWER +train_prompt_7\="The following are abstract and question about them. Each abstract is followed by a question and answer to a given question. Abstract: \{context\}\nQuestion: \{question\}\nAnswer:" +train_split_7=train +train_n_shot_7=0 +max_new_tokens_8=128 +train_dataset_8="keivalya/MedQuad-MedicalQnADataset" +train_text_column_8=Question +train_label_column_8=Answer +train_prompt_8\="Question: \{question\}\nAnswer: \{answer\}" +train_split_8=train +few_shot_split_8=train +train_n_shot_8=5 +max_new_tokens_9=107 +train_dataset_9="['wmt14', 'fr-en']" +train_text_column_9=fr +train_label_column_9=en +train_prompt_9\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_9=train +max_new_tokens_10=107 +train_dataset_10="['wmt19', 'de-en']" +train_text_column_10=de +train_label_column_10=en +train_prompt_10\="Here is a sentence in \{source_lang\} language and its translation in \{target_lang\} language.\n\nOriginal:\n\{text\}\nTranslation:\n" +train_split_10=train +run_baselines=False +run_eigenscore=False +run_layerwise_methods=True +md_device=cuda
wait